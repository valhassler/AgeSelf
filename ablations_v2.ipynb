{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5dad55c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from ageself.filter_faces_eyetracking_functions import smooth_running_median, build_eye_tracking_dataset\n",
    "import os\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import glob\n",
    "import numpy as np\n",
    "\n",
    "import plotly.graph_objs as go\n",
    "from plotly.subplots import make_subplots\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d301478",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base Face functions\n",
    "# Function to extract a single frame by index from a video\n",
    "def load_frame_at(path, frame_idx):\n",
    "    \"\"\"\n",
    "    Load a specific frame from the video without preloading all frames.\n",
    "\n",
    "    Args:\n",
    "        path (str): Path to the video file.\n",
    "        frame_idx (int): Zero-based index of the desired frame.\n",
    "\n",
    "    Returns:\n",
    "        frame (np.ndarray): BGR image array of the specified frame.\n",
    "    \"\"\"\n",
    "    cap = cv2.VideoCapture(path)\n",
    "    if not cap.isOpened():\n",
    "        raise IOError(f\"Cannot open video file {path}\")\n",
    "\n",
    "    # Seek to the frame index\n",
    "    cap.set(cv2.CAP_PROP_POS_FRAMES, frame_idx)\n",
    "    ret, frame = cap.read()\n",
    "    cap.release()\n",
    "    if not ret:\n",
    "        raise IndexError(f\"Frame {frame_idx} cannot be read\")\n",
    "    return frame\n",
    "\n",
    "# Function to plot a frame with bounding boxes\n",
    "def plot_frame_with_boxes(frame, annotations, figsize=(8, 6)):\n",
    "    \"\"\"\n",
    "    Plot a single video frame with rectangle annotations.\n",
    "\n",
    "    Args:\n",
    "        frame (np.ndarray): BGR image array.\n",
    "        annotations (pd.DataFrame): Subset of box_annotation_df for one frame.\n",
    "        figsize (tuple): Figure size.\n",
    "    \"\"\"\n",
    "    # Convert BGR to RGB\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    fig, ax = plt.subplots(1, figsize=figsize)\n",
    "    ax.imshow(frame_rgb)\n",
    "    ax.axis('off')\n",
    "\n",
    "    # Draw each box\n",
    "    for _, row in annotations.iterrows():\n",
    "        x, y, w, h = row.x_l, row.y_l, row.width, row.height\n",
    "        age = row.age_class\n",
    "        gender = row.gender\n",
    "        rect = patches.Rectangle(\n",
    "            (x, y), w, h,\n",
    "            linewidth=1,\n",
    "            edgecolor='red',\n",
    "            facecolor='none'\n",
    "        )\n",
    "        ax.add_patch(rect)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    return fig, ax\n",
    "\n",
    "def scale_boxes(annotations, scale_factor = 1):\n",
    "    \"\"\"\n",
    "    Scale bounding boxes in-place by a given factor, keeping the same center.\n",
    "\n",
    "    Args:\n",
    "        annotations (pd.DataFrame): DataFrame with columns [x_l, y_l, width, height].\n",
    "        scale_factor (float): Scale multiplier (e.g., 2.0 doubles box size).\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: New DataFrame with scaled x_l, y_l, width, height.\n",
    "    \"\"\"\n",
    "    df = annotations.copy().reset_index(drop=True)\n",
    "    # Calculate centers\n",
    "    cx = df.x_l + df.width / 2.0\n",
    "    cy = df.y_l + df.height / 2.0\n",
    "\n",
    "    # Scale sizes\n",
    "    new_w = df.width * scale_factor\n",
    "    new_h = df.height * scale_factor\n",
    "\n",
    "    # Compute new top-left\n",
    "    df['x_l'] = cx - new_w / 2.0\n",
    "    df['y_l'] = cy - new_h / 2.0\n",
    "    df['width'] = new_w\n",
    "    df['height'] = new_h\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# Eye Tracking FUnctions\n",
    "\n",
    "\n",
    "def plot_eye_tracking_data(data_eye_tracking):\n",
    "    fig = make_subplots(\n",
    "        rows=2, cols=1,\n",
    "        shared_xaxes=True,\n",
    "        row_heights=[0.5, 0.5],\n",
    "        vertical_spacing=0.1,\n",
    "        subplot_titles=(\"X Position over Time\", \"Y Position over Time\")\n",
    "    )\n",
    "\n",
    "    # X pos trace\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=data_eye_tracking[\"timestamp\"],\n",
    "            y=data_eye_tracking[\"pos_x\"],\n",
    "            mode=\"lines+markers\",\n",
    "            name=\"pos_x\"\n",
    "        ),\n",
    "        row=1, col=1\n",
    "    )\n",
    "\n",
    "    # Y pos trace\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=data_eye_tracking[\"timestamp\"],\n",
    "            y=data_eye_tracking[\"pos_y\"],\n",
    "            mode=\"lines+markers\",\n",
    "            name=\"pos_y\"\n",
    "        ),\n",
    "        row=2, col=1\n",
    "    )\n",
    "\n",
    "    # Layout tweaks\n",
    "    fig.update_layout(\n",
    "        height=600, width=800,\n",
    "        title=\"Interactive Eye‑Tracking → X and Y Positions\",\n",
    "        xaxis_title=\"Timestamp\",\n",
    "        yaxis=dict(title=\"Position\"),\n",
    "        hovermode=\"x unified\"\n",
    "    )\n",
    "\n",
    "    \n",
    "    # fig.update_yaxes(range=[-2000, 2000], row=1, col=1)\n",
    "    # fig.update_yaxes(range=[-2000, 2000], row=2, col=1)\n",
    "\n",
    "    # Show it!\n",
    "    fig.show()\n",
    "\n",
    "\n",
    "def make_lag_diff(data_eye_tracking: pd.DataFrame, lag=1) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Compute lag-n differences for X and Y eye‑tracking positions.\n",
    "\n",
    "    Args:\n",
    "        data_eye_tracking (pd.DataFrame):  \n",
    "            Must contain columns ['world_index', 'pos_x', 'pos_y'].\n",
    "        lag (int):  \n",
    "            Number of steps back to subtract (e.g. lag=1 gives frame-to-frame diffs).\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: with columns ['world_index', 'delta_x', 'delta_y'], length = len(data_eye_tracking) - lag\n",
    "    \"\"\"\n",
    "    df = data_eye_tracking.copy().reset_index(drop=True)\n",
    "\n",
    "    # Compute shifted series\n",
    "    df['pos_x_prev'] = df['pos_x'].shift(lag)\n",
    "    df['pos_y_prev'] = df['pos_y'].shift(lag)\n",
    "\n",
    "    # Compute deltas\n",
    "    df['pos_x'] = df['pos_x'] - df['pos_x_prev']\n",
    "    df['pos_y'] = df['pos_y'] - df['pos_y_prev']\n",
    "\n",
    "    # Align world_index to the current frame (the later one)\n",
    "    df_out = df.loc[lag:, ['world_index', 'pos_x', 'pos_y']].reset_index(drop=True)\n",
    "    return df_out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e42f27c",
   "metadata": {},
   "source": [
    "# Base Loading of required data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "852e6241",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = \"/usr/users/vhassle\"\n",
    "\n",
    "# Load seqs meta data\n",
    "information_raw_df = pd.read_csv(os.path.join(base_path,\"datasets/Wortschatzinsel/head_mounted_data/scene_view_creation_df.csv\"))\n",
    "decision_df = pd.read_csv(os.path.join(base_path,\"datasets/Wortschatzinsel/head_mounted_data/final_IDs.csv\"))\n",
    "# Create one dataset that has the annotations from the facedetection boxes and their according labels \n",
    "information_filtered_df = pd.merge(information_raw_df, decision_df, left_on=\"scene_view_nr\", right_on=\"ID\", how=\"inner\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b02615ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_eye_tracking_raw_path = os.path.join(base_path, \"datasets/Wortschatzinsel/head_mounted_data/eye_tracking_annotations/valid\")\n",
    "eye_tracking_raw_paths = glob.glob(os.path.join(base_eye_tracking_raw_path, \"*\", \"gaze_positions*.csv\"))\n",
    "eye_tracking_raw_paths.sort()\n",
    "seq_names = [eye_tracking_raw_path.split(\"/\")[-2] for eye_tracking_raw_path in eye_tracking_raw_paths]\n",
    "\n",
    "# idx 0 is definetly correct!\n",
    "\n",
    "idx = 0 # 15, 91, 2\n",
    "seq_name = seq_names[idx]\n",
    "\n",
    "video_path = os.path.join(base_path,\"datasets/Wortschatzinsel/head_mounted_data/videos/valid\", seq_name + \".mp4\")\n",
    "box_annotation_path = os.path.join(base_path,\"model_outputs/Wortschatzinsel/age_gender_classification_reversed_from_results\", seq_name + \".txt\")\n",
    "\n",
    "eye_tracking_raw_path = eye_tracking_raw_paths[idx]\n",
    "\n",
    "# Load detection annotations (boxes)\n",
    "box_annotation_df = pd.read_csv(\n",
    "    box_annotation_path,\n",
    "    header=None,\n",
    "    names=[\"frame\", \"face_nuber_on_frame\", \"x_l\", \"y_l\", \"width\", \"height\", \"n1\",\"n2\",\"n3\",\"n4\",\"age_class\", \"gender\"])\n",
    "\n",
    "box_annotation_df.frame = box_annotation_df.frame - 1\n",
    "\n",
    "\n",
    "# Load eyetracking data\n",
    "# check if pupilcore or neon\n",
    "is_neon = information_filtered_df[information_filtered_df.new_name == seq_name].neon.values[0]\n",
    "data_eye_tracking, gaze_df = build_eye_tracking_dataset(seq_name, video_path, eye_tracking_raw_path, base_eye_tracking_raw_path, is_neon, window_ms= 100)\n",
    "\n",
    "\n",
    "\n",
    "print(f\"Number of eyetracking annotations: {len(data_eye_tracking)}\")\n",
    "print(f\"seq_name : {seq_name}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f749d16c",
   "metadata": {},
   "source": [
    "# Look on video to see face size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd85c3f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Raw eye‑tracking data\n",
    "data_eye_tracking_subset = gaze_df#.iloc[1000:2000].copy()\n",
    "print(data_eye_tracking_subset.shape)\n",
    "plot_eye_tracking_data(data_eye_tracking_subset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85666c97",
   "metadata": {},
   "source": [
    "# Postprocessing\n",
    "\n",
    "- First group by frame of the video to get at least a constant frame rate (that is what is used so far)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91fdac4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_eye_tracking_raw_path = os.path.join(base_path, \"datasets/Wortschatzinsel/head_mounted_data/eye_tracking_annotations/valid\")\n",
    "eye_tracking_raw_paths = glob.glob(os.path.join(base_eye_tracking_raw_path, \"*\", \"gaze_positions*.csv\"))\n",
    "eye_tracking_raw_paths.sort()\n",
    "seq_names = [eye_tracking_raw_path.split(\"/\")[-2] for eye_tracking_raw_path in eye_tracking_raw_paths]\n",
    "\n",
    "# idx 0 is definetly correct!\n",
    "\n",
    "idx = 1 # 15, 91, 2\n",
    "seq_name = seq_names[idx]\n",
    "\n",
    "video_path = os.path.join(base_path,\"datasets/Wortschatzinsel/head_mounted_data/videos/valid\", seq_name + \".mp4\")\n",
    "box_annotation_path = os.path.join(base_path,\"model_outputs/Wortschatzinsel/age_gender_classification_reversed_from_results\", seq_name + \".txt\")\n",
    "\n",
    "eye_tracking_raw_path = eye_tracking_raw_paths[idx]\n",
    "\n",
    "# Load detection annotations (boxes)\n",
    "box_annotation_df = pd.read_csv(\n",
    "    box_annotation_path,\n",
    "    header=None,\n",
    "    names=[\"frame\", \"face_nuber_on_frame\", \"x_l\", \"y_l\", \"width\", \"height\", \"n1\",\"n2\",\"n3\",\"n4\",\"age_class\", \"gender\"])\n",
    "\n",
    "box_annotation_df.frame = box_annotation_df.frame - 1\n",
    "\n",
    "\n",
    "# Load eyetracking data\n",
    "# check if pupilcore or neon\n",
    "is_neon = information_filtered_df[information_filtered_df.new_name == seq_name].neon.values[0]\n",
    "data_eye_tracking, gaze_df = build_eye_tracking_dataset(seq_name, video_path, eye_tracking_raw_path, base_eye_tracking_raw_path, is_neon, window_ms= 100)\n",
    "\n",
    "\n",
    "\n",
    "print(f\"Number of eyetracking annotations: {len(data_eye_tracking)}\")\n",
    "print(f\"seq_name : {seq_name}\")\n",
    "\n",
    "\n",
    "data_eye_tracking_subset = data_eye_tracking.copy()#[1000:2000]\n",
    "data_eye_tracking_subset.timestamp = data_eye_tracking_subset.world_index\n",
    "plot_eye_tracking_data(data_eye_tracking_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5003441",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate lagged differences data plot\n",
    "\n",
    "data_eye_tracking_subset =  data_eye_tracking.copy()[1000:2000]\n",
    "print(data_eye_tracking_subset.shape)\n",
    "diff_data = make_lag_diff(data_eye_tracking_subset)\n",
    "diff_data[\"timestamp\"] = diff_data[\"world_index\"]\n",
    "plot_eye_tracking_data(diff_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d936abf",
   "metadata": {},
   "source": [
    "# Quantitive summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a17241b",
   "metadata": {},
   "source": [
    "### Movement of foucs (How far looks the person per Frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9271169",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_eye_tracking_raw_path = os.path.join(base_path, \"datasets/Wortschatzinsel/head_mounted_data/eye_tracking_annotations/valid\")\n",
    "eye_tracking_raw_paths = glob.glob(os.path.join(base_eye_tracking_raw_path, \"*\", \"gaze_positions*.csv\"))\n",
    "eye_tracking_raw_paths.sort()\n",
    "seq_names = [eye_tracking_raw_path.split(\"/\")[-2] for eye_tracking_raw_path in eye_tracking_raw_paths]\n",
    "seq_names.sort()\n",
    "\n",
    "box_annotation_paths = glob.glob(os.path.join(base_path,\"model_outputs/Wortschatzinsel/age_gender_classification_reversed_from_results\",\"*.txt\"))\n",
    "box_annotation_paths.sort()\n",
    "\n",
    "moving_table = []\n",
    "for eye_tracking_raw_path, seq_name, box_annotation_path in tqdm.tqdm(zip(eye_tracking_raw_paths, seq_names, box_annotation_paths)):\n",
    "    assert seq_name in box_annotation_path and seq_name in eye_tracking_raw_path, f\"Mismatch in seq_name: {seq_name} not found in paths.\"\n",
    "    # load the box annotations\n",
    "    box_annotation_df = pd.read_csv(\n",
    "        box_annotation_path,\n",
    "        header=None,\n",
    "        names=[\"frame\", \"face_nuber_on_frame\", \"x_l\", \"y_l\", \"width\", \"height\", \"n1\",\"n2\",\"n3\",\"n4\",\"age_class\", \"gender\"])\n",
    "\n",
    "    # check if pupilcore or neon\n",
    "    is_neon = information_filtered_df[information_filtered_df.new_name == seq_name].neon.values[0]\n",
    "    data_per_frame_eye, gaze_df = build_eye_tracking_dataset(seq_name, video_path, eye_tracking_raw_path, base_eye_tracking_raw_path, is_neon, window_ms= 100)\n",
    "\n",
    "    moving = []\n",
    "    moving.append(seq_name)\n",
    "    moving.append(is_neon)\n",
    "\n",
    "    box_eye_annotation_df = pd.merge(box_annotation_df, data_per_frame_eye, how='outer', left_on='frame', right_on='world_index')\n",
    "    box_eye_annotation_df['frame'] = box_eye_annotation_df['frame'].combine_first(box_eye_annotation_df['world_index'])\n",
    "\n",
    "    #optional smooth it for the statistics\n",
    "    # if not is_neon:\n",
    "    #     box_eye_annotation_df = smooth_running_mean(box_eye_annotation_df, window_ms=300, hz=30)\n",
    "\n",
    "    #calculate the difference between two timeframes:\n",
    "    box_lagged = make_lag_diff(box_eye_annotation_df, lag=1)\n",
    "    movement_x = box_lagged[\"pos_x\"].abs().mean()\n",
    "    movement_y = box_lagged[\"pos_y\"].abs().mean()\n",
    "    moving.append(movement_x)\n",
    "    moving.append(movement_y)\n",
    "\n",
    "    moving_table.append(moving)\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dfde8e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "overview_moving = pd.DataFrame(moving_table, columns=[\"seq_name\", \"is_neon\", \"x_change\", \"y_change\"])\n",
    "means = overview_moving.drop(columns=[\"seq_name\"]).groupby(\"is_neon\").mean()\n",
    "\n",
    "print(means)\n",
    "\n",
    "fig, axes = plt.subplots(2, 1, figsize=(10, 8), tight_layout=True)\n",
    "for neon_flag, df_group in overview_moving.groupby(\"is_neon\"):\n",
    "    label = f\"is_neon={neon_flag}\"\n",
    "    # x_change histogram on top row\n",
    "    axes[0].hist(df_group[\"x_change\"], bins=30, alpha=0.6, label=label)\n",
    "    # y_change histogram on bottom row\n",
    "    axes[1].hist(df_group[\"y_change\"], bins=30, alpha=0.6, label=label)\n",
    "\n",
    "# customize axes\n",
    "axes[0].set_title(\"Histogram of x_change by is_neon\")\n",
    "axes[1].set_title(\"Histogram of y_change by is_neon\")\n",
    "for ax in axes:\n",
    "    ax.set_xlabel(\"Change value\")\n",
    "    ax.set_ylabel(\"Frequency\")\n",
    "    ax.legend()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b95cd975",
   "metadata": {},
   "source": [
    "## Validity measures applied on Videos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4300fa16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, glob, tqdm, numpy as np, pandas as pd\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# gather files\n",
    "# ------------------------------------------------------------------\n",
    "base_eye_tracking_raw_path = os.path.join(\n",
    "    base_path,\n",
    "    \"datasets/Wortschatzinsel/head_mounted_data/eye_tracking_annotations/valid\"\n",
    ")\n",
    "eye_tracking_raw_paths = sorted(\n",
    "    glob.glob(os.path.join(base_eye_tracking_raw_path, \"*\", \"gaze_positions*.csv\"))\n",
    ")\n",
    "seq_names = sorted([p.split(\"/\")[-2] for p in eye_tracking_raw_paths])\n",
    "\n",
    "box_annotation_paths = sorted(\n",
    "    glob.glob(os.path.join(\n",
    "        base_path,\n",
    "        \"model_outputs/Wortschatzinsel/age_gender_classification_reversed_from_results\",\n",
    "        \"*.txt\"))\n",
    ")\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# metrics per video\n",
    "# ------------------------------------------------------------------\n",
    "metrics_table = []           # each row: [seq_name, is_neon, n_empty_tail, pct_zero_change, pct_longest_zero]\n",
    "\n",
    "for eye_path, seq_name, box_path in tqdm.tqdm(zip(eye_tracking_raw_paths,\n",
    "                                                  seq_names,\n",
    "                                                  box_annotation_paths)):\n",
    "    assert seq_name in box_path and seq_name in eye_path, f\"path mismatch: {seq_name}\"\n",
    "\n",
    "    # -------------------------\n",
    "    # build gaze‑data per frame\n",
    "    # -------------------------\n",
    "    is_neon = information_filtered_df.loc[\n",
    "        information_filtered_df.new_name == seq_name, \"neon\"\n",
    "    ].values[0]\n",
    "\n",
    "    data_eye_tracking, gaze_df = build_eye_tracking_dataset(\n",
    "        seq_name, video_path, eye_path, base_eye_tracking_raw_path,\n",
    "        is_neon, window_ms=0\n",
    "    )\n",
    "\n",
    "    # -------------------------------------------------------------\n",
    "    # 1) valid mask: both coordinates present\n",
    "    # -------------------------------------------------------------\n",
    "    valid_mask = data_eye_tracking[[\"pos_x\", \"pos_y\"]].notna().all(axis=1)\n",
    "    valid_df   = data_eye_tracking.loc[valid_mask].copy()\n",
    "\n",
    "    # -------------------------------------------------------------\n",
    "    # 1. empty‑tail frames\n",
    "    # -------------------------------------------------------------\n",
    "    max_valid_idx   = valid_df[\"world_index\"].max()\n",
    "    max_world_index = data_eye_tracking[\"world_index\"].max()\n",
    "    n_empty_tail    = int(max_world_index - max_valid_idx)\n",
    "\n",
    "    # -------------------------------------------------------------\n",
    "    # 2. percentage of repeated gaze points\n",
    "    # -------------------------------------------------------------\n",
    "    zero_change = (\n",
    "        (valid_df[\"pos_x\"].diff().fillna(0) == 0) &\n",
    "        (valid_df[\"pos_y\"].diff().fillna(0) == 0)\n",
    "    )\n",
    "    pct_zero_change = 100 * zero_change.mean()\n",
    "\n",
    "    # -------------------------------------------------------------\n",
    "    # 3. longest continuous zero‑change stretch (percent of valid)\n",
    "    # -------------------------------------------------------------\n",
    "    groups           = zero_change.ne(zero_change.shift()).cumsum()\n",
    "    longest_stretch  = zero_change.groupby(groups).sum().max() or 0\n",
    "    pct_longest_zero = 100 * longest_stretch / len(valid_df)\n",
    "\n",
    "    # -------------------------------------------------------------\n",
    "    # collect row\n",
    "    # -------------------------------------------------------------\n",
    "    metrics_table.append([\n",
    "        seq_name,\n",
    "        bool(is_neon),\n",
    "        n_empty_tail,\n",
    "        pct_zero_change,\n",
    "        pct_longest_zero\n",
    "    ])\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# final DataFrame\n",
    "# ------------------------------------------------------------------\n",
    "cols = [\"seq_name\", \"is_neon\", \"empty_tail_frames\", \"pct_zero_change\", \"pct_longest_zero\"]\n",
    "metrics_df = pd.DataFrame(metrics_table, columns=cols)\n",
    "print(metrics_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a76d4e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_rows\", 200)\n",
    "metrics_df.head(91)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3046d2c9",
   "metadata": {},
   "source": [
    "### Compare diferent bounding boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baafdf45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import tqdm\n",
    "base_path_bounding_boxes = \"/usr/users/vhassle/model_outputs/Wortschatzinsel/detection_tracking_merged_v04/age_gender_classification_reversed_from_results\"\n",
    "\n",
    "\n",
    "result_table_ts = []\n",
    "result_table_fn = []\n",
    "for seq_name in tqdm.tqdm((seq_names)):\n",
    "    paths = glob.glob(os.path.join(base_path_bounding_boxes, f\"*{seq_name}*.csv\"))\n",
    "    paths.sort()\n",
    "\n",
    "    result_row_ts = []\n",
    "    result_row_fn = []\n",
    "    result_row_ts.append(int(seq_name.split(\"_\")[0]))\n",
    "    result_row_fn.append(int(seq_name.split(\"_\")[0]))\n",
    "\n",
    "    for path in paths:\n",
    "        result = pd.read_csv(path)\n",
    "        # drop where timestamp is NaN\n",
    "        result = result.dropna(subset=['timestamp'])\n",
    "        per_frame_result = result.groupby('frame', as_index=False)['eye_in_box'].max()\n",
    "        mean_cover = per_frame_result.eye_in_box.mean()\n",
    "        result_row_ts.append(per_frame_result.eye_in_box.mean())\n",
    "\n",
    "        result = result.dropna(subset=['face_nuber_on_frame'])\n",
    "        per_frame_result = result.groupby('frame', as_index=False)['eye_in_box'].max()\n",
    "        mean_cover = per_frame_result.eye_in_box.mean()\n",
    "        result_row_fn.append(per_frame_result.eye_in_box.mean())   \n",
    "\n",
    "        \n",
    "    result_table_ts.append(result_row_ts)\n",
    "    result_table_fn.append(result_row_fn)\n",
    "        \n",
    "\n",
    "result_table_df_ts = pd.DataFrame(result_table_ts, columns=[\"seq_name\", \"0\", \"5\", \"10\",\"15\", \"20\", \"30\", \"40\", \"50\", \"60\", \"80\", \"100\"])\n",
    "result_table_df_fn = pd.DataFrame(result_table_fn, columns=[\"seq_name\", \"0\", \"5\", \"10\",\"15\", \"20\", \"30\", \"40\", \"50\", \"60\", \"80\", \"100\"])\n",
    "\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caff1ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6598fa90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "# result_table_df_fn, result_table_df_ts\n",
    "result_table_df = result_table_df_ts\n",
    "result_table_df[\"is_neon\"] = (result_table_df['seq_name'] >= 400) & (result_table_df['seq_name'] < 500)\n",
    "result_table_df\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 1) Compute means per is_neon (dropping the seq_name column)\n",
    "means = result_table_df.drop(columns=['seq_name']).groupby('is_neon').mean()\n",
    "print(means)\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 0)  Collect depth columns and convert to numeric order\n",
    "# ------------------------------------------------------------------\n",
    "coverage_cols = (\n",
    "    result_table_df                # your DataFrame\n",
    "      .columns\n",
    "      .difference(['seq_name', 'is_neon'])   # keep only depth columns\n",
    "      .tolist()\n",
    ")\n",
    "coverage_cols = sorted(coverage_cols, key=lambda x: int(x))   # numeric sort\n",
    "\n",
    "x_vals = np.array([int(c) for c in coverage_cols])            # numeric x‑axis\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 1)  Compute required quantiles per class\n",
    "# ------------------------------------------------------------------\n",
    "quant_levels = [0, 0.25, 0.5, 0.75, 1.0]\n",
    "quant = (\n",
    "    result_table_df\n",
    "      .groupby('is_neon')[coverage_cols]\n",
    "      .quantile(quant_levels)        # MultiIndex (is_neon, quantile)\n",
    ")\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 2)  Plot median line + shaded quantile bands\n",
    "# ------------------------------------------------------------------\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "for is_neon_flag, color in zip([False, True], ['tab:blue', 'tab:orange']):\n",
    "    q0   = quant.loc[(is_neon_flag, 0.00)][coverage_cols]\n",
    "    q25  = quant.loc[(is_neon_flag, 0.25)][coverage_cols]\n",
    "    q50  = quant.loc[(is_neon_flag, 0.50)][coverage_cols]\n",
    "    q75  = quant.loc[(is_neon_flag, 0.75)][coverage_cols]\n",
    "    q100 = quant.loc[(is_neon_flag, 1.00)][coverage_cols]\n",
    "\n",
    "    # outer (min‑max) band\n",
    "    ax.fill_between(x_vals, q0, q100, color=color, alpha=0.10)\n",
    "    # inner (IQR) band\n",
    "    ax.fill_between(x_vals, q25, q75, color=color, alpha=0.25)\n",
    "    # median line\n",
    "    ax.plot(x_vals, q50, color=color, marker='o', label=f'Neon = {is_neon_flag}')\n",
    "\n",
    "    #mean line\n",
    "    mean_vals = result_table_df.loc[result_table_df['is_neon'] == is_neon_flag, coverage_cols].mean()\n",
    "    ax.plot(x_vals, mean_vals, color=color, linestyle='--', label=f'Mean (Neon = {is_neon_flag})')\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 3)  Cosmetics\n",
    "# ------------------------------------------------------------------\n",
    "ax.set_xlabel('Sequencing depth category')\n",
    "ax.set_ylabel('Coverage')\n",
    "ax.set_title('Coverage distribution by depth (median + quantile bands)')\n",
    "ax.grid(True, which='both', linestyle='--', linewidth=0.5, alpha=0.6)\n",
    "ax.legend(title='Group')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a654dd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "curiosity",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
