{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "import numpy as np\n",
    "import glob\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from ageself.annotate_videos_functions import VideoDataset\n",
    "import cv2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This can be used to use the to improve model training by creating new images that can be downstream classified for next training itteration.\n",
    "# In general I would rather suggest to use the model from classification (age, gender) to apply this for detection and therefore having not this two step approach where we first cut out the faces and then do classification. but \n",
    "# rather to all of them in one step\n",
    "\n",
    "base_path_videos = \"/usr/users/vhassle/datasets/Wortschatzinsel/all_videos\"\n",
    "annotation_pahts = sorted(glob.glob(\"/usr/users/vhassle/model_outputs/outputs_AgeSelf/age_gender_classification_model_final/*002.txt\"))\n",
    "video_paths = sorted([os.path.join(base_path_videos, os.path.basename(annotation_paht).replace(\".txt\", \".mp4\").replace(\"_r002\", \"\")) for annotation_paht in annotation_pahts])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotation_table_all = pd.DataFrame()\n",
    "for annotation_path, video_path in zip(annotation_pahts, video_paths):\n",
    "    try:\n",
    "        annotation_table = pd.read_csv(annotation_path, sep=\",\", header=None)\n",
    "    except:\n",
    "        print(\"Error reading\", annotation_path, \"probably empty\")\n",
    "        continue\n",
    "    annotation_table[\"video_path\"] = video_path\n",
    "    annotation_table_all = pd.concat([annotation_table_all, annotation_table])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotation_table_all.columns = [\"frame\", \"face_nr\", \"x\", \"y\", \"w\", \"h\",\"\",\"\",\"\",\"\",\"age\",\"gender\",\"video_path\"]\n",
    "subset_annotation_table = annotation_table_all[(annotation_table_all[\"w\"] * annotation_table_all[\"h\"]) > 4900]\n",
    "print(subset_annotation_table.shape)\n",
    "anotation_subsample =  subset_annotation_table.sample(n=1000, random_state=42)\n",
    "print(anotation_subsample.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_video_paths = anotation_subsample[\"video_path\"].unique()\n",
    "annotations_image_crops = []\n",
    "\n",
    "output_dir = \"/usr/users/vhassle/datasets/Wortschatzinsel/face_crops\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "for sampled_video_path in tqdm(sampled_video_paths):\n",
    "    video_dataset = VideoDataset(sampled_video_path)\n",
    "    anotation_subsamples_video = anotation_subsample[anotation_subsample[\"video_path\"] == sampled_video_path]\n",
    "\n",
    "    # Process each annotation in the video\n",
    "    for annotation_subsample_video in anotation_subsamples_video.iterrows():\n",
    "        frame = annotation_subsample_video[1][\"frame\"]\n",
    "        x = annotation_subsample_video[1][\"x\"]\n",
    "        y = annotation_subsample_video[1][\"y\"]\n",
    "        w = annotation_subsample_video[1][\"w\"]\n",
    "        h = annotation_subsample_video[1][\"h\"]\n",
    "        age = annotation_subsample_video[1][\"age\"]\n",
    "        gender = annotation_subsample_video[1][\"gender\"]\n",
    "        \n",
    "        # Cut out face from the frame\n",
    "        face = video_dataset[frame][y:y+h, x:x+w]\n",
    "        \n",
    "        # Save face image with a unique name\n",
    "        image_name = f\"face_{sampled_video_path.split('/')[-1].split('.')[0]}_frame{frame}_x{x}_y{y}_w{w}_h{h}.jpg\"\n",
    "        image_path = os.path.join(\"Wortschatzinsel/face_crops\", image_name)\n",
    "        save_image_path = os.path.join(output_dir, image_name)\n",
    "        face = cv2.cvtColor(face, cv2.COLOR_BGR2RGB) #it is the other way but leads to the same result\n",
    "        cv2.imwrite(save_image_path, face)\n",
    "        \n",
    "        # Append annotation with image name and folder, age, and gender\n",
    "        annotations_image_crops.append({\n",
    "            \"image_path\": image_path,\n",
    "            \"age\": age,\n",
    "            \"gender\": gender\n",
    "        })\n",
    "\n",
    "# Save annotations to a text file\n",
    "annotations_file_path = os.path.join(output_dir, \"wortschatz_faces.txt\")\n",
    "with open(annotations_file_path, \"w\") as f:\n",
    "    for annotation in annotations_image_crops:\n",
    "        f.write(f\"{annotation['image_path']},{annotation['age']},{annotation['gender']}\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
