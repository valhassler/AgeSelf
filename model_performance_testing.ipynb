{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torchvision import models, transforms\n",
    "from PIL import Image\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import glob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResizeToMaxDim:\n",
    "    def __init__(self, max_size):\n",
    "        self.max_size = max_size\n",
    "    \n",
    "    def __call__(self, img):\n",
    "        # Get current size\n",
    "        w, h = img.size\n",
    "        if w > h:\n",
    "            new_w = self.max_size\n",
    "            new_h = int(h * (self.max_size / w))\n",
    "        else:\n",
    "            new_h = self.max_size\n",
    "            new_w = int(w * (self.max_size / h))\n",
    "        return img.resize((new_w, new_h), Image.LANCZOS)\n",
    "    \n",
    "class PadToSquare:\n",
    "    def __init__(self, size, fill=0):\n",
    "        self.size = size\n",
    "        self.fill = fill\n",
    "    \n",
    "    def __call__(self, img):\n",
    "        w, h = img.size\n",
    "        pad_w = (self.size - w) // 2\n",
    "        pad_h = (self.size - h) // 2\n",
    "        padding = (pad_w, pad_h, self.size - w - pad_w, self.size - h - pad_h)\n",
    "        return transforms.functional.pad(img, padding, fill=self.fill)\n",
    "\n",
    "\n",
    "def plot_image_with_age(model, image_path, csv_data = None, classification = False, image_size = 448):\n",
    "    # Define transformation\n",
    "\n",
    "    \n",
    "    # transform = transforms.Compose([\n",
    "    #     transforms.Resize(448),\n",
    "    #     #transforms.CenterCrop(600),\n",
    "    #     transforms.ToTensor(),\n",
    "    #     transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    # ])\n",
    "\n",
    "    transform = transforms.Compose([\n",
    "            ResizeToMaxDim(image_size),\n",
    "            PadToSquare(image_size),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ])\n",
    "\n",
    "    # Load and preprocess the image\n",
    "    if isinstance(image_path, str):\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "    else:\n",
    "        image = Image.fromarray(image_path)\n",
    "\n",
    "\n",
    "    input_image_before_cuda = transform(image)\n",
    "    # plt.imshow(input_image_before_cuda.permute(1, 2, 0))\n",
    "    # plt.show()\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    input_image = input_image_before_cuda.unsqueeze(0).to(device)\n",
    "\n",
    "    # Get the true age from the CSVnot so clean writtenm since lag_benchmark_cropped is not the best way to make the comparison slower ways I have seen, but couldn come up with similar fast\n",
    "    if csv_data is not None:\n",
    "        true_age = csv_data[csv_data['img_name'] == os.path.join(\"lag_benchmark_cropped\",os.path.basename(image_path))]['age'].values[0]\n",
    "    else:\n",
    "        true_age = -1\n",
    "\n",
    "    # Make prediction\n",
    "    # plt.imshow(input_image_before_cuda.numpy().transpose(1, 2, 0))\n",
    "    # plt.show()\n",
    "    with torch.no_grad():\n",
    "        output = model(input_image)\n",
    "        if not classification:\n",
    "            predicted_age = output.item()\n",
    "        else:\n",
    "            predicted_age = output\n",
    "       \n",
    "    # Plot the image with the predicted and true age\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.imshow(image)\n",
    "\n",
    "    if classification:\n",
    "        predicted_age_group = nn.Softmax(dim=1)(output)\n",
    "        # predicted_age_group = np.argmax(output.cpu().numpy())\n",
    "        # age_group_mapping = {\n",
    "        #     0: '0-2',\n",
    "        #     1: '3-13',\n",
    "        #     2: '18-99',\n",
    "        # }\n",
    "        # predicted_age_group = age_group_mapping.get(predicted_age_group, 'Unknown')\n",
    "        plt.title(f'Estimated Age: {predicted_age_group}\\nTrue Age: {true_age}')\n",
    "\n",
    "    else:\n",
    "        plt.title(f'Estimated Age: {predicted_age:.2f}\\nTrue Age: {true_age}')\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Age Regression whole Body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model weights\n",
    "\n",
    "model_weights_path = '/usr/users/vhassle/psych_track/AgeSelf/models/age_classification_model_25.pth'\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')  \n",
    "model_age = models.resnet50(pretrained=False)\n",
    "num_ftrs = model_age.fc.in_features\n",
    "model_age.fc = nn.Linear(num_ftrs, 1)\n",
    "model_age.load_state_dict(torch.load(model_weights_path))\n",
    "model = model_age.to(device)\n",
    "model.eval()\n",
    "\n",
    "# Load true ages from CSV\n",
    "data = pd.read_csv('/usr/users/vhassle/datasets/lagenda/cropped_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "#image_paths = glob.glob(\"/usr/users/vhassle/datasets/lagenda/lag_benchmark_cropped/*\")\n",
    "data_subset = data[data['age'] != 100]\n",
    "image_paths = [os.path.join('/usr/users/vhassle/datasets/lagenda', image_name) for image_name in data_subset['img_name']]\n",
    "\n",
    "print(len(image_paths))\n",
    "for image_path in image_paths[2134:2135]:\n",
    "    plot_image_with_age(model_age, image_path, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Age classification whole Body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model weights\n",
    "# model_weights_path = '/usr/users/vhassle/datasets/lagenda/age_classification_model.pth'\n",
    "model_weights_path = '/usr/users/vhassle/psych_track/AgeSelf/models/age_classification_model_15_focal_pad.pth'\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')  \n",
    "model_age = models.resnet50(pretrained=False)\n",
    "num_ftrs = model_age.fc.in_features\n",
    "model_age.fc = nn.Linear(num_ftrs, 3)\n",
    "model_age.load_state_dict(torch.load(model_weights_path))\n",
    "model = model_age.to(device)\n",
    "model.eval()\n",
    "\n",
    "# Load true ages from CSV\n",
    "data_class = pd.read_csv('/usr/users/vhassle/datasets/lagenda/cropped_data_age_classes.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_subset = data[data['age'] != 100]\n",
    "image_paths = [os.path.join('/usr/users/vhassle/datasets/lagenda', image_name) for image_name in data_subset['img_name']]\n",
    "\n",
    "print(len(image_paths))\n",
    "for image_path in image_paths[0:100:100]:\n",
    "    plot_image_with_age(model_age, image_path, data, classification=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#image_paths = glob.glob(\"/usr/users/vhassle/psych_track/Gender-and-Age-Detection_Face/*.jpg\")\n",
    "image_paths = glob.glob(\"/usr/users/vhassle/datasets/Wortschatzinsel/cropped_images/*.jpg\")\n",
    "for image_path in image_paths:\n",
    "    plot_image_with_age(model_age, image_path, None, classification=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FÃ¼r die Gesichter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load model weights\n",
    "# model_weights_path = '/usr/users/vhassle/datasets/lagenda/age_classification_model.pth'\n",
    "model_weights_path = '/usr/users/vhassle/psych_track/AgeSelf/models/age_classification_model_15_focal_pad.pth'\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')  \n",
    "model_age = models.resnet50(pretrained=False)\n",
    "num_ftrs = model_age.fc.in_features\n",
    "model_age.fc = nn.Linear(num_ftrs, 3)\n",
    "model_age.load_state_dict(torch.load(model_weights_path))\n",
    "model_age = model_age.to(device)\n",
    "model_age.eval()\n",
    "# Load true ages from CSV\n",
    "data_class = pd.read_csv('/usr/users/vhassle/datasets/lagenda/cropped_data_age_classes.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#image_paths = glob.glob(\"/usr/users/vhassle/psych_track/Gender-and-Age-Detection_Face/*.jpg\")\n",
    "#image_paths = glob.glob(\"/usr/users/vhassle/datasets/Wortschatzinsel/cropped_images/*.jpg\")\n",
    "image_paths = glob.glob(\"/usr/users/vhassle/datasets/example_images/children/*\")\n",
    "for image_path in image_paths[0:1]:\n",
    "    plot_image_with_age(model_age, image_path, None, classification=True, image_size=150)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## combining face detection with age classification in a video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import decord\n",
    "class VideoDataset(Dataset):\n",
    "    def __init__(self, video_path):\n",
    "        # Initialize the VideoReader\n",
    "        self.vr = decord.VideoReader(video_path, ctx=decord.cpu(0))  # Load video in CPU memory\n",
    "        self.length = len(self.vr)  # Total number of frames\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if idx < 0 or idx >= self.length:\n",
    "            raise IndexError(\"Index out of bounds\")\n",
    "        frame = self.vr[idx].asnumpy()\n",
    "        return frame #output: top_view, coming_in_view, going_out_view\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "video_1 = VideoDataset(\"/usr/users/vhassle/datasets/Wortschatzinsel/Neon_complete/Neon/2024-05-04-11-30-31/2024_05_04_11_30_31.mp4\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame = video_1[5000]\n",
    "\n",
    "\n",
    "frame = Image.open(\"/usr/users/vhassle/datasets/example_images/children/1000_F_140125186_ktTYm8WJ8EDK7Fc82g9A9OU3OrTa5cyg.jpg\")\n",
    "frame = np.asarray(frame)\n",
    "plt.imshow(frame)\n",
    "plt.show()\n",
    "from pytorch_retinaface.detect import process_image, load_Retinanet\n",
    "model_age = load_Retinanet(\"/usr/users/vhassle/psych_track/Pytorch_Retinaface/Resnet50_Final.pth\")\n",
    "faces = process_image(model, frame)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function that does the stuff on its own"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# V1 works 4.5 img/s on phobos right now 1 img/s on deimos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import cv2\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import decord\n",
    "from torchvision import models, transforms\n",
    "from pytorch_retinaface.detect import process_image, load_Retinanet #self created module self installed\n",
    "import os \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"7\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResizeToMaxDim:\n",
    "    def __init__(self, max_size):\n",
    "        self.max_size = max_size\n",
    "    \n",
    "    def __call__(self, img):\n",
    "        # Get current size\n",
    "        w, h = img.size\n",
    "        if w > h:\n",
    "            new_w = self.max_size\n",
    "            new_h = int(h * (self.max_size / w))\n",
    "        else:\n",
    "            new_h = self.max_size\n",
    "            new_w = int(w * (self.max_size / h))\n",
    "        return img.resize((new_w, new_h), Image.LANCZOS)\n",
    "    \n",
    "class PadToSquare:\n",
    "    def __init__(self, size, fill=0):\n",
    "        self.size = size\n",
    "        self.fill = fill\n",
    "    \n",
    "    def __call__(self, img):\n",
    "        w, h = img.size\n",
    "        pad_w = (self.size - w) // 2\n",
    "        pad_h = (self.size - h) // 2\n",
    "        padding = (pad_w, pad_h, self.size - w - pad_w, self.size - h - pad_h)\n",
    "        return transforms.functional.pad(img, padding, fill=self.fill)\n",
    "\n",
    "\n",
    "def plot_image_with_estimated_ages(image, model_age, model_face_detection ,index_frame = 0, classification=False, image_size=448):\n",
    "    \"\"\"\n",
    "    Plots an image with estimated ages annotated on detected faces.\n",
    "\n",
    "    Parameters:\n",
    "    image (Union[str, np.ndarray]): Path to the image or a NumPy array representing the image.\n",
    "    model: The model used for age estimation.\n",
    "    classification (bool): If True, classify the age group. If False, predict the exact age.\n",
    "    image_size (int): The size to which the image should be resized.\n",
    "    \"\"\"\n",
    "    # Define transformation\n",
    "    transform = transforms.Compose([\n",
    "        ResizeToMaxDim(image_size),\n",
    "        PadToSquare(image_size),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "\n",
    "    # Load the image\n",
    "    if isinstance(image, str):\n",
    "        frame = cv2.imread(image)\n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    elif isinstance(image, np.ndarray):\n",
    "        frame_rgb = image\n",
    "        frame = cv2.cvtColor(frame_rgb, cv2.COLOR_RGB2BGR)\n",
    "    else:\n",
    "        raise ValueError(\"image must be a file path or a numpy array\")\n",
    "\n",
    "    # Detect faces\n",
    "    faces = process_image(model_face_detection, frame_rgb)\n",
    "    height, width, _ = frame.shape\n",
    "\n",
    "    # Prepare for prediction\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    annotations = []\n",
    "\n",
    "    for face_key in faces.keys():\n",
    "        face_area = faces[face_key]\n",
    "        x1, y1, x2, y2 = face_area\n",
    "        x1, y1, x2, y2 = max(0, x1), max(0, y1), min(width, x2), min(height, y2)\n",
    "\n",
    "        # Extract the face\n",
    "        face = frame_rgb[y1:y2, x1:x2]\n",
    "\n",
    "        # Convert the face to PIL image and apply transformation\n",
    "        face_pil = Image.fromarray(face)\n",
    "        input_image_before_cuda = transform(face_pil)\n",
    "        input_image = input_image_before_cuda.unsqueeze(0).to(device)\n",
    "\n",
    "        # Make prediction\n",
    "        with torch.no_grad():\n",
    "            output = model_age(input_image)\n",
    "            if not classification:\n",
    "                predicted_age = output.item()\n",
    "                predicted_age_text = f'Age: {predicted_age:.2f}'\n",
    "            else:\n",
    "                predicted_age_group = nn.Softmax(dim=1)(output)\n",
    "                age_group = predicted_age_group.argmax(dim=1).item()\n",
    "                predicted_age_text = f'{age_group}'\n",
    "\n",
    "        # Annotate the image using OpenCV\n",
    "        cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "        cv2.putText(frame, predicted_age_text, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n",
    "\n",
    "        # Prepare annotation for MOT format\n",
    "        annotation = [index_frame, face_key, x1, y1, x2 - x1, y2 - y1, 1, -1, -1, -1, age_group]\n",
    "        annotations.append(annotation)\n",
    "\n",
    "    return frame, annotations\n",
    "\n",
    "# Adapted DataLoader\n",
    "class VideoDataset(Dataset):\n",
    "    def __init__(self, video_path):\n",
    "        \"\"\"\n",
    "        Args: video_path (str): Path to the video file\n",
    "        \"\"\"\n",
    "        # Initialize the VideoReader\n",
    "        self.vr = decord.VideoReader(video_path, ctx=decord.cpu(0))  # Load video in CPU memory\n",
    "        self.length = len(self.vr)  # Total number of frames\n",
    "        self.video_size = self.vr[0].asnumpy().shape\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        frame = self.vr[idx].asnumpy()\n",
    "        return frame\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "# Process video and save annotated video\n",
    "def process_video(video_path, model_age, model_face_detection, output_video_path, output_annotations_path, classification=False, image_size=448):\n",
    "    dataset = VideoDataset(video_path)\n",
    "    dataloader = DataLoader(dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "    video_writer = None\n",
    "    annotations = []\n",
    "\n",
    "    for idx, frame in enumerate(tqdm(dataloader)):\n",
    "        if idx > 50:\n",
    "            break\n",
    "        frame = frame.squeeze(0).numpy()\n",
    "        annotated_frame, frame_annotations = plot_image_with_estimated_ages(image = frame, model_age = model_age,model_face_detection=  model_face_detection, index_frame = idx, classification = classification, image_size = image_size)\n",
    "\n",
    "        # Initialize video writer\n",
    "        if video_writer is None:\n",
    "            height, width, _ = annotated_frame.shape\n",
    "            fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "            video_writer = cv2.VideoWriter(output_video_path, fourcc, 30, (width, height))\n",
    "\n",
    "        video_writer.write(cv2.cvtColor(annotated_frame, cv2.COLOR_RGB2BGR))\n",
    "        for annotation in frame_annotations:\n",
    "            annotation[0] = idx  # Set frame index\n",
    "            annotations.append(annotation)\n",
    "\n",
    "    video_writer.release()\n",
    "\n",
    "    # Save annotations in MOT style\n",
    "    with open(output_annotations_path, 'w') as f:\n",
    "        for annotation in annotations:\n",
    "            f.write(','.join(map(str, annotation)) + '\\n')\n",
    "\n",
    "    print(f\"Annotated video saved to {output_video_path}\")\n",
    "    print(f\"Annotations saved to {output_annotations_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_path = \"/usr/users/vhassle/datasets/Wortschatzinsel/Neon_complete/Neon/2024-05-04-11-14-21/2024_05_04_11_14_21.mp4\"\n",
    "output_video_path = \"/usr/users/vhassle/2024_05_04_11_14_21_faces.mp4\"\n",
    "output_annotations_path = \"/usr/users/vhassle/2024_05_04_11_14_21_annotation.txt\"\n",
    "\n",
    "\n",
    "model_weights_path = '/usr/users/vhassle/psych_track/AgeSelf/models/age_classification_model_15_focal_pad.pth'\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')  \n",
    "model_age = models.resnet50(pretrained=False)\n",
    "num_ftrs = model_age.fc.in_features\n",
    "model_age.fc = nn.Linear(num_ftrs, 3)\n",
    "model_age.load_state_dict(torch.load(model_weights_path))\n",
    "model_age = model_age.to(device)\n",
    "model_age.eval()\n",
    "\n",
    "model_face_detection = load_Retinanet(\"/usr/users/vhassle/psych_track/Pytorch_Retinaface/Resnet50_Final.pth\")\n",
    "model_face_detection = model_face_detection.to(device)\n",
    "\n",
    "\n",
    "process_video(video_path = video_path, model_age = model_age, model_face_detection= model_face_detection, output_video_path = output_video_path, output_annotations_path = output_annotations_path, classification=True, image_size=150)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# V2 not working yet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"7\"\n",
    "import cv2\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import decord\n",
    "from torchvision import models, transforms\n",
    "from pytorch_retinaface.detect import process_image, load_Retinanet #self created module self installed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_path = [\"/usr/users/vhassle/datasets/Wortschatzinsel/raspi_wsi1_2024_05_19_000.mp4\"]\n",
    "dataloader =  decord.VideoLoader(video_path, ctx=[decord.cpu(0)], shape=(10, 1200, 1600, 3), interval=0, skip=0, shuffle=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "b = a[0].permute(0,3,1,2).float().to(device)\n",
    "c = b[0:1,:,:,:]\n",
    "c.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1):\n",
    "    model_face_detection(torch.rand(20,3,1200,1600).to(\"cuda\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(100):\n",
    "    model_face_detection(c+i)\n",
    "\n",
    "# Kaum Beschleunigung durch parallele Verarbeitung hier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hier gehts richtig los\n",
    "class ResizeToMaxDim:\n",
    "    def __init__(self, max_size):\n",
    "        self.max_size = max_size\n",
    "    \n",
    "    def __call__(self, img):\n",
    "        w, h = img.size\n",
    "        if w > h:\n",
    "            new_w = self.max_size\n",
    "            new_h = int(h * (self.max_size / w))\n",
    "        else:\n",
    "            new_h = self.max_size\n",
    "            new_w = int(w * (self.max_size / h))\n",
    "        return img.resize((new_w, new_h), Image.LANCZOS)\n",
    "\n",
    "class PadToSquare:\n",
    "    def __init__(self, size, fill=0):\n",
    "        self.size = size\n",
    "        self.fill = fill\n",
    "    \n",
    "    def __call__(self, img):\n",
    "        w, h = img.size\n",
    "        pad_w = (self.size - w) // 2\n",
    "        pad_h = (self.size - h) // 2\n",
    "        padding = (pad_w, pad_h, self.size - w - pad_w, self.size - h - pad_h)\n",
    "        return F.pad(img, padding, fill=self.fill)\n",
    "\n",
    "def process_image_batch(image, model_age, model_face_detection, index_frame=0, classification=False, image_size=150):\n",
    "    transform = transforms.Compose([\n",
    "        ResizeToMaxDim(image_size),\n",
    "        PadToSquare(image_size),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "\n",
    "    #model_age needs RGB\n",
    "    # model_face_detection needs BGR\n",
    "\n",
    "    faces = process_image(model_face_detection, frame_rgb)\n",
    "    height, width, _ = frame.shape\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    annotations = []\n",
    "\n",
    "    for face_key in faces.keys():\n",
    "        face_area = faces[face_key]\n",
    "        x1, y1, x2, y2 = face_area\n",
    "        x1, y1, x2, y2 = max(0, x1), max(0, y1), min(width, x2), min(height, y2)\n",
    "\n",
    "        face = frame_rgb[y1:y2, x1:x2]\n",
    "        face_pil = Image.fromarray(face)\n",
    "        input_image_before_cuda = transform(face_pil)\n",
    "        input_image = input_image_before_cuda.unsqueeze(0).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output = model_age(input_image)\n",
    "            if not classification:\n",
    "                predicted_age = output.item()\n",
    "                predicted_age_text = f'Age: {predicted_age:.2f}'\n",
    "            else:\n",
    "                predicted_age_group = nn.Softmax(dim=1)(output)\n",
    "                age_group = predicted_age_group.argmax(dim=1).item()\n",
    "                predicted_age_text = f'{age_group}'\n",
    "\n",
    "        cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "        cv2.putText(frame, predicted_age_text, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n",
    "\n",
    "        annotation = [index_frame, face_key, x1, y1, x2 - x1, y2 - y1, 1, -1, -1, -1]\n",
    "        annotations.append(annotation)\n",
    "\n",
    "    return frame, annotations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Set Decord to use PyTorch tensors\n",
    "class VideoDataset(Dataset):\n",
    "    def __init__(self, video_paths):\n",
    "        self.vl = VideoLoader(video_paths, ctx=[cpu(0)], shape=(2, 320, 240, 3), interval=1, skip=5, shuffle=1)\n",
    "        self.length = len(self.vl)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        batch = self.vl[idx]\n",
    "        return batch[0], idx\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "def process_video(video_path, model_age, model_face_detection, output_video_path, output_annotations_path, classification=False, image_size_face=150):\n",
    "    #dataset = VideoDataset(video_path)\n",
    "    #dataloader = DataLoader(dataset, batch_size=1, shuffle=False, num_workers=num_workers)\n",
    "    decord.bridge.set_bridge('torch')\n",
    "    dataloader =  decord.VideoLoader([video_path], ctx=[decord.cpu(0)], shape=(10, 1200, 1600, 3), interval=0, skip=0, shuffle=0)    \n",
    "    \n",
    "    video_writer = None\n",
    "    annotations = []\n",
    "\n",
    "    for frames_batch in tqdm(dataloader):\n",
    "        frames, idx = frames_batch\n",
    "        idx = [int(id[1]) for id in idx]\n",
    "\n",
    "        frames = frames.permute(0, 3, 1, 2).numpy()  # Convert from BTCHW to BCHW\n",
    "        frames = frames.device('cuda')\n",
    "\n",
    "        annotated_frame, frame_annotations = image_with_estimated_ages(frame, model_age, model_face_detection, idx, classification, image_size_face)\n",
    "\n",
    "    #     if video_writer is None:\n",
    "    #         height, width, _ = annotated_frame.shape\n",
    "    #         fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    #         video_writer = cv2.VideoWriter(output_video_path, fourcc, 30, (width, height))\n",
    "\n",
    "    #     video_writer.write(cv2.cvtColor(annotated_frame, cv2.COLOR_RGB2BGR))\n",
    "    #     for annotation in frame_annotations:\n",
    "    #         annotation[0] = idx\n",
    "    #         annotations.append(annotation)\n",
    "\n",
    "    # video_writer.release()\n",
    "\n",
    "    # with open(output_annotations_path, 'w') as f:\n",
    "    #     for annotation in annotations:\n",
    "    #         f.write(','.join(map(str, annotation)) + '\\n')\n",
    "\n",
    "    # print(f\"Annotated video saved to {output_video_path}\")\n",
    "    # print(f\"Annotations saved to {output_annotations_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_path = \"/usr/users/vhassle/datasets/Wortschatzinsel/Neon_complete/Neon/2024-05-04-11-14-21/2024_05_04_11_14_21.mp4\"\n",
    "output_video_path = \"/usr/users/vhassle/2024_05_04_11_14_21_faces.mp4\"\n",
    "output_annotations_path = \"/usr/users/vhassle/2024_05_04_11_14_21_annotation.txt\"\n",
    "\n",
    "\n",
    "model_weights_path = '/usr/users/vhassle/psych_track/AgeSelf/models/age_classification_model_15_focal_pad.pth'\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')  \n",
    "model_age = models.resnet50(pretrained=False)\n",
    "num_ftrs = model_age.fc.in_features\n",
    "model_age.fc = nn.Linear(num_ftrs, 3)\n",
    "model_age.load_state_dict(torch.load(model_weights_path))\n",
    "model_age = model_age.to(device)\n",
    "model_age.eval()\n",
    "\n",
    "model_face_detection = load_Retinanet(\"/usr/users/vhassle/psych_track/Pytorch_Retinaface/Resnet50_Final.pth\")\n",
    "\n",
    "process_video(video_path, model_age, model_face_detection, output_video_path, output_annotations_path, classification=True, image_size_face=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
